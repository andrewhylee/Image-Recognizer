{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Task for Today\n***\n## Using Data about Korean Person, Predict his/her income."},{"metadata":{},"cell_type":"markdown","source":"\n## 1. Setting Up"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, HuberRegressor\nfrom sklearn.svm import LinearSVR\nfrom sklearn.tree import DecisionTreeRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/korea-income-and-welfare/Korea Income and Welfare.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data.id.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = data.copy()\n\n# df = df.replace(\" \", np.nan)\n\ndf.isna().sum()\n# This shows that we can replace whitespace values with 0's\n# as that is the appropriate replacement value for absence of occupation, absence of company_size\n# and absence of reason_none_worker.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df['company_size'] == ' ']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.replace(' ', 0)\ndf[df['company_size'] == ' ']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing_data(df):\n    # Make copy of dataframe to not change the original\n    df = df.copy()\n    \n    # Drop ID column for simplification\n    df = df.drop('id', axis=1)\n\n    # The only \"N/A\" values are whitespaces\n    # Replace them with values of 0\n    df = df.replace(\" \", 0)\n    \n    # Make company_size from object into int type\n    df.company_size = df.company_size.astype(int)\n    \n    # Create \"Employed\" Column\n    df['employed'] = df.occupation != 0\n    df['employed'] = df['employed'].astype(int)\n    \n    # Categorical columns and their prefixes\n    nominal = [\n        ('region', 'reg'),\n        ('gender', 'gen'),\n        ('marriage', 'marr'),\n        ('occupation', 'occ'),\n        ('reason_none_worker', 'non-work'),\n\n    ]\n    \n    # One-Hot Encoding Categorical Features\n    for column, prefix in nominal:\n        df = onehot_encode(df, column, prefix)\n    \n    # Split data into input and label (X and y)\n    y = df.income\n    X = df.drop('income', axis=1)\n    \n    # Train and test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)\n    \n    # Scale\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = preprocessing_data(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    \"              LinearRegression\":LinearRegression(),\n    \"L2 Regularized RidgeRegression\":Ridge(),\n    \"L1 Regularized LassoRegression\":Lasso(),\n    \"                HuberRegressor\":HuberRegressor(),\n    \"                     LinearSVR\":LinearSVR(),\n    \"         DecisionTreeRegressor\":DecisionTreeRegressor(),\n}","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"{'              LinearRegression': LinearRegression(),\n 'L2 Regularized RidgeRegression': Ridge(),\n 'L1 Regularized LassoRegression': Lasso(),\n '                HuberRegressor': HuberRegressor(),\n '                     LinearSVR': LinearSVR(),\n '         DecisionTreeRegressor': DecisionTreeRegressor()}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"models.items()","execution_count":83,"outputs":[{"output_type":"execute_result","execution_count":83,"data":{"text/plain":"dict_items([('              LinearRegression', LinearRegression()), ('L2 Regularized RidgeRegression', Ridge()), ('L1 Regularized LassoRegression', Lasso()), ('                HuberRegressor', HuberRegressor()), ('                     LinearSVR', LinearSVR()), ('         DecisionTreeRegressor', DecisionTreeRegressor())])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, model in models.items():\n    model.fit(X_train,y_train)\n    print(\"The {} model is trained\".format(name) )","execution_count":84,"outputs":[{"output_type":"stream","text":"The               LinearRegression model is trained\nThe L2 Regularized RidgeRegression model is trained\nThe L1 Regularized LassoRegression model is trained\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_huber.py:296: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","name":"stderr"},{"output_type":"stream","text":"The                 HuberRegressor model is trained\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n","name":"stderr"},{"output_type":"stream","text":"The                      LinearSVR model is trained\nThe          DecisionTreeRegressor model is trained\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 4. Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"r_sq = model.score(X_test, y_test)\nprint(\"R^2 value for All Models: \")\n\nfor name, model in models.items():\n    score = model.score(X_test, y_test)\n    print(name, \" : {:.5f}\".format(score) )","execution_count":85,"outputs":[{"output_type":"stream","text":"R^2 value for All Models: \n              LinearRegression  : 0.22286\nL2 Regularized RidgeRegression  : 0.22301\nL1 Regularized LassoRegression  : 0.22154\n                HuberRegressor  : 0.19086\n                     LinearSVR  : 0.20403\n         DecisionTreeRegressor  : -0.01024\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 5. Optimizing Regularization Strength of L1 and L2 Regression Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# default alpha value is 1.0\n# \n# Not only minimizes the cost, but the square of the weights of each feature\n#\n# alpha value of 0 is same as normal linear regression\nl2_model = Ridge(alpha=0.1)\nl2_model.fit(X_train, y_train)\nprint(\"L2 Reg. Model Score {}\".format(l2_model.score(X_test,y_test)) )","execution_count":90,"outputs":[{"output_type":"stream","text":"L2 Reg. Model Score 0.22288091025341905\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# L1 allows automatic feature selection\n# by lowering the absolute value of the weight\n# lowering bigger weights is same as lowering smaller weights\n# Basically a shift: The smaller weights can actually be set all the way down to 0\nl1_model = Lasso(alpha=1.0)","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Lin=LinearRegression()\nLin.fit(X_train,y_train)\nprint(Lin.score(X_test, y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}