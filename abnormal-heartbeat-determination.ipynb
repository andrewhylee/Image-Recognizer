{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Task for Today\n***\n### Predict whether the patient has a abnormal Heart or not"},{"metadata":{},"cell_type":"markdown","source":"## 1. Setting Up"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs = [pd.read_csv('../input/heartbeat/ptbdb_' + i + '.csv') for i in ['normal', 'abnormal'] ]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change Column Headers to Sequential Whole Numbers\nfor df in dfs:\n    df.columns = (range(len(df.columns)))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs[1]","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"            0         1         2         3         4         5         6    \\\n0      1.000000  0.606941  0.384181  0.254237  0.223567  0.276836  0.253430   \n1      1.000000  0.951613  0.923963  0.853303  0.791859  0.734255  0.672043   \n2      0.977819  0.899261  0.230129  0.032348  0.142329  0.223660  0.328096   \n3      0.935618  0.801661  0.805815  1.000000  0.722741  0.480789  0.454829   \n4      0.925265  0.433352  0.073620  0.079197  0.136643  0.182934  0.182934   \n...         ...       ...       ...       ...       ...       ...       ...   \n10500  0.981409  1.000000  0.559171  0.287093  0.196639  0.204862  0.215946   \n10501  0.906250  0.922379  0.878024  0.810484  0.712702  0.667339  0.608871   \n10502  1.000000  0.867971  0.674122  0.470332  0.296987  0.169307  0.077664   \n10503  1.000000  0.984672  0.658888  0.556394  0.446809  0.395790  0.315260   \n10504  0.997886  0.700317  0.464059  0.318182  0.233615  0.184989  0.124207   \n\n            7         8         9    ...  178  179  180  181  182  183  184  \\\n0      0.184826  0.153349  0.121872  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1      0.685100  0.670507  0.667435  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2      0.367837  0.381701  0.389094  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3      0.319834  0.266874  0.308411  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4      0.182376  0.196877  0.203569  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n10500  0.243833  0.242760  0.250268  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n10501  0.527218  0.480847  0.442540  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n10502  0.081392  0.074868  0.089779  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n10503  0.276367  0.261039  0.258522  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n10504  0.082981  0.087738  0.063953  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       185  186  187  \n0      0.0  0.0  1.0  \n1      0.0  0.0  1.0  \n2      0.0  0.0  1.0  \n3      0.0  0.0  1.0  \n4      0.0  0.0  1.0  \n...    ...  ...  ...  \n10500  0.0  0.0  1.0  \n10501  0.0  0.0  1.0  \n10502  0.0  0.0  1.0  \n10503  0.0  0.0  1.0  \n10504  0.0  0.0  1.0  \n\n[10505 rows x 188 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>178</th>\n      <th>179</th>\n      <th>180</th>\n      <th>181</th>\n      <th>182</th>\n      <th>183</th>\n      <th>184</th>\n      <th>185</th>\n      <th>186</th>\n      <th>187</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.606941</td>\n      <td>0.384181</td>\n      <td>0.254237</td>\n      <td>0.223567</td>\n      <td>0.276836</td>\n      <td>0.253430</td>\n      <td>0.184826</td>\n      <td>0.153349</td>\n      <td>0.121872</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>0.951613</td>\n      <td>0.923963</td>\n      <td>0.853303</td>\n      <td>0.791859</td>\n      <td>0.734255</td>\n      <td>0.672043</td>\n      <td>0.685100</td>\n      <td>0.670507</td>\n      <td>0.667435</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.977819</td>\n      <td>0.899261</td>\n      <td>0.230129</td>\n      <td>0.032348</td>\n      <td>0.142329</td>\n      <td>0.223660</td>\n      <td>0.328096</td>\n      <td>0.367837</td>\n      <td>0.381701</td>\n      <td>0.389094</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.935618</td>\n      <td>0.801661</td>\n      <td>0.805815</td>\n      <td>1.000000</td>\n      <td>0.722741</td>\n      <td>0.480789</td>\n      <td>0.454829</td>\n      <td>0.319834</td>\n      <td>0.266874</td>\n      <td>0.308411</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.925265</td>\n      <td>0.433352</td>\n      <td>0.073620</td>\n      <td>0.079197</td>\n      <td>0.136643</td>\n      <td>0.182934</td>\n      <td>0.182934</td>\n      <td>0.182376</td>\n      <td>0.196877</td>\n      <td>0.203569</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10500</th>\n      <td>0.981409</td>\n      <td>1.000000</td>\n      <td>0.559171</td>\n      <td>0.287093</td>\n      <td>0.196639</td>\n      <td>0.204862</td>\n      <td>0.215946</td>\n      <td>0.243833</td>\n      <td>0.242760</td>\n      <td>0.250268</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10501</th>\n      <td>0.906250</td>\n      <td>0.922379</td>\n      <td>0.878024</td>\n      <td>0.810484</td>\n      <td>0.712702</td>\n      <td>0.667339</td>\n      <td>0.608871</td>\n      <td>0.527218</td>\n      <td>0.480847</td>\n      <td>0.442540</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10502</th>\n      <td>1.000000</td>\n      <td>0.867971</td>\n      <td>0.674122</td>\n      <td>0.470332</td>\n      <td>0.296987</td>\n      <td>0.169307</td>\n      <td>0.077664</td>\n      <td>0.081392</td>\n      <td>0.074868</td>\n      <td>0.089779</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10503</th>\n      <td>1.000000</td>\n      <td>0.984672</td>\n      <td>0.658888</td>\n      <td>0.556394</td>\n      <td>0.446809</td>\n      <td>0.395790</td>\n      <td>0.315260</td>\n      <td>0.276367</td>\n      <td>0.261039</td>\n      <td>0.258522</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>10504</th>\n      <td>0.997886</td>\n      <td>0.700317</td>\n      <td>0.464059</td>\n      <td>0.318182</td>\n      <td>0.233615</td>\n      <td>0.184989</td>\n      <td>0.124207</td>\n      <td>0.082981</td>\n      <td>0.087738</td>\n      <td>0.063953</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10505 rows × 188 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine and Shuffle the 2 DataFrames\ndata = pd.DataFrame()\nfor df in dfs:\n    data = pd.concat([data, df], axis=0)\n\ndata = data.sample(frac=1.0).reset_index(drop=True)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"            0         1         2         3         4         5         6    \\\n0      1.000000  0.597252  0.277657  0.045553  0.124367  0.106291  0.167751   \n1      1.000000  0.562273  0.269649  0.065296  0.001209  0.021765  0.010883   \n2      0.964692  0.953303  0.550683  0.426538  0.317768  0.281891  0.305809   \n3      0.987293  0.527147  0.215248  0.028879  0.057374  0.110127  0.121679   \n4      1.000000  0.864491  0.737832  0.352323  0.258296  0.248894  0.196903   \n...         ...       ...       ...       ...       ...       ...       ...   \n14545  0.995739  0.824031  0.227098  0.031104  0.072007  0.141031  0.141457   \n14546  1.000000  0.638404  0.562594  0.501247  0.534663  0.483791  0.443392   \n14547  0.963631  1.000000  0.790803  0.615570  0.529907  0.536219  0.543733   \n14548  1.000000  0.700777  0.428506  0.213339  0.000914  0.055276  0.089539   \n14549  1.000000  0.710864  0.366822  0.169393  0.121495  0.086449  0.116238   \n\n            7         8         9    ...  178  179  180  181  182  183  184  \\\n0      0.152567  0.126537  0.181490  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1      0.022370  0.033857  0.007255  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2      0.309225  0.299544  0.291572  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3      0.105506  0.102811  0.100501  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4      0.243916  0.211283  0.179757  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n14545  0.133788  0.137196  0.132510  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14546  0.367082  0.400000  0.425436  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14547  0.561767  0.547941  0.534115  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14548  0.090452  0.081772  0.081772  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14549  0.106308  0.122664  0.127336  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       185  186  187  \n0      0.0  0.0  1.0  \n1      0.0  0.0  1.0  \n2      0.0  0.0  1.0  \n3      0.0  0.0  0.0  \n4      0.0  0.0  1.0  \n...    ...  ...  ...  \n14545  0.0  0.0  0.0  \n14546  0.0  0.0  1.0  \n14547  0.0  0.0  1.0  \n14548  0.0  0.0  0.0  \n14549  0.0  0.0  1.0  \n\n[14550 rows x 188 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>178</th>\n      <th>179</th>\n      <th>180</th>\n      <th>181</th>\n      <th>182</th>\n      <th>183</th>\n      <th>184</th>\n      <th>185</th>\n      <th>186</th>\n      <th>187</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.597252</td>\n      <td>0.277657</td>\n      <td>0.045553</td>\n      <td>0.124367</td>\n      <td>0.106291</td>\n      <td>0.167751</td>\n      <td>0.152567</td>\n      <td>0.126537</td>\n      <td>0.181490</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>0.562273</td>\n      <td>0.269649</td>\n      <td>0.065296</td>\n      <td>0.001209</td>\n      <td>0.021765</td>\n      <td>0.010883</td>\n      <td>0.022370</td>\n      <td>0.033857</td>\n      <td>0.007255</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.964692</td>\n      <td>0.953303</td>\n      <td>0.550683</td>\n      <td>0.426538</td>\n      <td>0.317768</td>\n      <td>0.281891</td>\n      <td>0.305809</td>\n      <td>0.309225</td>\n      <td>0.299544</td>\n      <td>0.291572</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.987293</td>\n      <td>0.527147</td>\n      <td>0.215248</td>\n      <td>0.028879</td>\n      <td>0.057374</td>\n      <td>0.110127</td>\n      <td>0.121679</td>\n      <td>0.105506</td>\n      <td>0.102811</td>\n      <td>0.100501</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.000000</td>\n      <td>0.864491</td>\n      <td>0.737832</td>\n      <td>0.352323</td>\n      <td>0.258296</td>\n      <td>0.248894</td>\n      <td>0.196903</td>\n      <td>0.243916</td>\n      <td>0.211283</td>\n      <td>0.179757</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14545</th>\n      <td>0.995739</td>\n      <td>0.824031</td>\n      <td>0.227098</td>\n      <td>0.031104</td>\n      <td>0.072007</td>\n      <td>0.141031</td>\n      <td>0.141457</td>\n      <td>0.133788</td>\n      <td>0.137196</td>\n      <td>0.132510</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14546</th>\n      <td>1.000000</td>\n      <td>0.638404</td>\n      <td>0.562594</td>\n      <td>0.501247</td>\n      <td>0.534663</td>\n      <td>0.483791</td>\n      <td>0.443392</td>\n      <td>0.367082</td>\n      <td>0.400000</td>\n      <td>0.425436</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14547</th>\n      <td>0.963631</td>\n      <td>1.000000</td>\n      <td>0.790803</td>\n      <td>0.615570</td>\n      <td>0.529907</td>\n      <td>0.536219</td>\n      <td>0.543733</td>\n      <td>0.561767</td>\n      <td>0.547941</td>\n      <td>0.534115</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14548</th>\n      <td>1.000000</td>\n      <td>0.700777</td>\n      <td>0.428506</td>\n      <td>0.213339</td>\n      <td>0.000914</td>\n      <td>0.055276</td>\n      <td>0.089539</td>\n      <td>0.090452</td>\n      <td>0.081772</td>\n      <td>0.081772</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14549</th>\n      <td>1.000000</td>\n      <td>0.710864</td>\n      <td>0.366822</td>\n      <td>0.169393</td>\n      <td>0.121495</td>\n      <td>0.086449</td>\n      <td>0.116238</td>\n      <td>0.106308</td>\n      <td>0.122664</td>\n      <td>0.127336</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>14550 rows × 188 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.rename({187: 'Labels'}, axis=1)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"              0         1         2         3         4         5         6  \\\n0      1.000000  0.597252  0.277657  0.045553  0.124367  0.106291  0.167751   \n1      1.000000  0.562273  0.269649  0.065296  0.001209  0.021765  0.010883   \n2      0.964692  0.953303  0.550683  0.426538  0.317768  0.281891  0.305809   \n3      0.987293  0.527147  0.215248  0.028879  0.057374  0.110127  0.121679   \n4      1.000000  0.864491  0.737832  0.352323  0.258296  0.248894  0.196903   \n...         ...       ...       ...       ...       ...       ...       ...   \n14545  0.995739  0.824031  0.227098  0.031104  0.072007  0.141031  0.141457   \n14546  1.000000  0.638404  0.562594  0.501247  0.534663  0.483791  0.443392   \n14547  0.963631  1.000000  0.790803  0.615570  0.529907  0.536219  0.543733   \n14548  1.000000  0.700777  0.428506  0.213339  0.000914  0.055276  0.089539   \n14549  1.000000  0.710864  0.366822  0.169393  0.121495  0.086449  0.116238   \n\n              7         8         9  ...  178  179  180  181  182  183  184  \\\n0      0.152567  0.126537  0.181490  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1      0.022370  0.033857  0.007255  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2      0.309225  0.299544  0.291572  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3      0.105506  0.102811  0.100501  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4      0.243916  0.211283  0.179757  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n14545  0.133788  0.137196  0.132510  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14546  0.367082  0.400000  0.425436  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14547  0.561767  0.547941  0.534115  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14548  0.090452  0.081772  0.081772  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n14549  0.106308  0.122664  0.127336  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n       185  186  Labels  \n0      0.0  0.0     1.0  \n1      0.0  0.0     1.0  \n2      0.0  0.0     1.0  \n3      0.0  0.0     0.0  \n4      0.0  0.0     1.0  \n...    ...  ...     ...  \n14545  0.0  0.0     0.0  \n14546  0.0  0.0     1.0  \n14547  0.0  0.0     1.0  \n14548  0.0  0.0     0.0  \n14549  0.0  0.0     1.0  \n\n[14550 rows x 188 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>178</th>\n      <th>179</th>\n      <th>180</th>\n      <th>181</th>\n      <th>182</th>\n      <th>183</th>\n      <th>184</th>\n      <th>185</th>\n      <th>186</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.000000</td>\n      <td>0.597252</td>\n      <td>0.277657</td>\n      <td>0.045553</td>\n      <td>0.124367</td>\n      <td>0.106291</td>\n      <td>0.167751</td>\n      <td>0.152567</td>\n      <td>0.126537</td>\n      <td>0.181490</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.000000</td>\n      <td>0.562273</td>\n      <td>0.269649</td>\n      <td>0.065296</td>\n      <td>0.001209</td>\n      <td>0.021765</td>\n      <td>0.010883</td>\n      <td>0.022370</td>\n      <td>0.033857</td>\n      <td>0.007255</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.964692</td>\n      <td>0.953303</td>\n      <td>0.550683</td>\n      <td>0.426538</td>\n      <td>0.317768</td>\n      <td>0.281891</td>\n      <td>0.305809</td>\n      <td>0.309225</td>\n      <td>0.299544</td>\n      <td>0.291572</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.987293</td>\n      <td>0.527147</td>\n      <td>0.215248</td>\n      <td>0.028879</td>\n      <td>0.057374</td>\n      <td>0.110127</td>\n      <td>0.121679</td>\n      <td>0.105506</td>\n      <td>0.102811</td>\n      <td>0.100501</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.000000</td>\n      <td>0.864491</td>\n      <td>0.737832</td>\n      <td>0.352323</td>\n      <td>0.258296</td>\n      <td>0.248894</td>\n      <td>0.196903</td>\n      <td>0.243916</td>\n      <td>0.211283</td>\n      <td>0.179757</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>14545</th>\n      <td>0.995739</td>\n      <td>0.824031</td>\n      <td>0.227098</td>\n      <td>0.031104</td>\n      <td>0.072007</td>\n      <td>0.141031</td>\n      <td>0.141457</td>\n      <td>0.133788</td>\n      <td>0.137196</td>\n      <td>0.132510</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14546</th>\n      <td>1.000000</td>\n      <td>0.638404</td>\n      <td>0.562594</td>\n      <td>0.501247</td>\n      <td>0.534663</td>\n      <td>0.483791</td>\n      <td>0.443392</td>\n      <td>0.367082</td>\n      <td>0.400000</td>\n      <td>0.425436</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14547</th>\n      <td>0.963631</td>\n      <td>1.000000</td>\n      <td>0.790803</td>\n      <td>0.615570</td>\n      <td>0.529907</td>\n      <td>0.536219</td>\n      <td>0.543733</td>\n      <td>0.561767</td>\n      <td>0.547941</td>\n      <td>0.534115</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>14548</th>\n      <td>1.000000</td>\n      <td>0.700777</td>\n      <td>0.428506</td>\n      <td>0.213339</td>\n      <td>0.000914</td>\n      <td>0.055276</td>\n      <td>0.089539</td>\n      <td>0.090452</td>\n      <td>0.081772</td>\n      <td>0.081772</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>14549</th>\n      <td>1.000000</td>\n      <td>0.710864</td>\n      <td>0.366822</td>\n      <td>0.169393</td>\n      <td>0.121495</td>\n      <td>0.086449</td>\n      <td>0.116238</td>\n      <td>0.106308</td>\n      <td>0.122664</td>\n      <td>0.127336</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>14550 rows × 188 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## 2. Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data.Labels.copy()\nX = data.drop('Labels', axis=1).copy()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=123)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"(10185, 187)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs = tf.keras.Input( shape=(X_train.shape[1],) )\n\nx = tf.keras.layers.Dense(64, activation='relu')(inputs)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ],\n)\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=4,\n            restore_best_weights=True,\n        )\n    ]\n)","execution_count":16,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n255/255 [==============================] - 1s 6ms/step - loss: 0.4653 - accuracy: 0.7703 - auc: 0.8103 - val_loss: 0.3829 - val_accuracy: 0.8228 - val_auc: 0.8797\nEpoch 2/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.3794 - accuracy: 0.8224 - auc: 0.8861 - val_loss: 0.3358 - val_accuracy: 0.8562 - val_auc: 0.9115\nEpoch 3/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.3337 - accuracy: 0.8516 - auc: 0.9153 - val_loss: 0.2978 - val_accuracy: 0.8841 - val_auc: 0.9325\nEpoch 4/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.2943 - accuracy: 0.8744 - auc: 0.9352 - val_loss: 0.2864 - val_accuracy: 0.8773 - val_auc: 0.9355\nEpoch 5/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.2609 - accuracy: 0.8960 - auc: 0.9494 - val_loss: 0.2585 - val_accuracy: 0.8930 - val_auc: 0.9479\nEpoch 6/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.2372 - accuracy: 0.9034 - auc: 0.9583 - val_loss: 0.2393 - val_accuracy: 0.8925 - val_auc: 0.9596\nEpoch 7/100\n255/255 [==============================] - 1s 5ms/step - loss: 0.2126 - accuracy: 0.9164 - auc: 0.9668 - val_loss: 0.2136 - val_accuracy: 0.9151 - val_auc: 0.9650\nEpoch 8/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1920 - accuracy: 0.9244 - auc: 0.9730 - val_loss: 0.1940 - val_accuracy: 0.9254 - val_auc: 0.9705\nEpoch 9/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1762 - accuracy: 0.9302 - auc: 0.9773 - val_loss: 0.1833 - val_accuracy: 0.9308 - val_auc: 0.9738\nEpoch 10/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1641 - accuracy: 0.9369 - auc: 0.9801 - val_loss: 0.2021 - val_accuracy: 0.9224 - val_auc: 0.9729\nEpoch 11/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1566 - accuracy: 0.9395 - auc: 0.9814 - val_loss: 0.1793 - val_accuracy: 0.9327 - val_auc: 0.9745\nEpoch 12/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1432 - accuracy: 0.9470 - auc: 0.9844 - val_loss: 0.1725 - val_accuracy: 0.9381 - val_auc: 0.9769\nEpoch 13/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1384 - accuracy: 0.9488 - auc: 0.9854 - val_loss: 0.1602 - val_accuracy: 0.9465 - val_auc: 0.9788\nEpoch 14/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1299 - accuracy: 0.9536 - auc: 0.9871 - val_loss: 0.1508 - val_accuracy: 0.9504 - val_auc: 0.9800\nEpoch 15/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1165 - accuracy: 0.9567 - auc: 0.9896 - val_loss: 0.1502 - val_accuracy: 0.9529 - val_auc: 0.9813\nEpoch 16/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1135 - accuracy: 0.9600 - auc: 0.9896 - val_loss: 0.1597 - val_accuracy: 0.9455 - val_auc: 0.9822\nEpoch 17/100\n255/255 [==============================] - 1s 5ms/step - loss: 0.1077 - accuracy: 0.9632 - auc: 0.9908 - val_loss: 0.1540 - val_accuracy: 0.9440 - val_auc: 0.9812\nEpoch 18/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1031 - accuracy: 0.9629 - auc: 0.9913 - val_loss: 0.1568 - val_accuracy: 0.9499 - val_auc: 0.9812\nEpoch 19/100\n255/255 [==============================] - 1s 4ms/step - loss: 0.1032 - accuracy: 0.9634 - auc: 0.9915 - val_loss: 0.1521 - val_accuracy: 0.9475 - val_auc: 0.9834\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 4. Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test, verbose=0)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Test Accuracy: {:.2f}%'.format(results[1]))\nprint('     Test AUC: {:.4f}'.format(results[2]))","execution_count":18,"outputs":[{"output_type":"stream","text":"Test Accuracy: 0.94%\n     Test AUC: 0.9804\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 5. Alt Approach: Training Using Recurrent NN"},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.expand_dims(X_train, axis=2).shape","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"TensorShape([10185, 187, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputs1 = tf.keras.Input( shape=(X_train.shape[1],) )\n\n# GRU Requires 3 dimensional input. Right now we have 1-D input\nexpand = tf.expand_dims(inputs1, axis=2)\n\n# One Example of LSTN Network: GRU\n# return_sequences=True will return multiple vectors (matrix)\ngru = tf.keras.layers.GRU(256, return_sequences=True)(expand)\n\n# Gotta flatten the matrix into vector\nflatten = tf.keras.layers.Flatten()(gru)\n\noutputs1 = tf.keras.layers.Dense(1, activation='sigmoid')(flatten)\n\nmodel = tf.keras.Model(inputs=inputs1, outputs=outputs1)\nprint(model.summary())","execution_count":23,"outputs":[{"output_type":"stream","text":"Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 187)]             0         \n_________________________________________________________________\ntf_op_layer_ExpandDims_2 (Te [(None, 187, 1)]          0         \n_________________________________________________________________\ngru_2 (GRU)                  (None, 187, 256)          198912    \n_________________________________________________________________\nflatten (Flatten)            (None, 47872)             0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 47873     \n=================================================================\nTotal params: 246,785\nTrainable params: 246,785\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ],\n)\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=4,\n            restore_best_weights=True,\n        )\n    ]\n)","execution_count":24,"outputs":[{"output_type":"stream","text":"Epoch 1/100\n255/255 [==============================] - 4s 18ms/step - loss: 0.4715 - accuracy: 0.7636 - auc: 0.8060 - val_loss: 0.4233 - val_accuracy: 0.8022 - val_auc: 0.8573\nEpoch 2/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.4233 - accuracy: 0.7944 - auc: 0.8526 - val_loss: 0.4043 - val_accuracy: 0.8193 - val_auc: 0.8825\nEpoch 3/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.3810 - accuracy: 0.8262 - auc: 0.8853 - val_loss: 0.3623 - val_accuracy: 0.8120 - val_auc: 0.9038\nEpoch 4/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.3451 - accuracy: 0.8500 - auc: 0.9080 - val_loss: 0.3260 - val_accuracy: 0.8596 - val_auc: 0.9182\nEpoch 5/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.3156 - accuracy: 0.8613 - auc: 0.9236 - val_loss: 0.3018 - val_accuracy: 0.8606 - val_auc: 0.9261\nEpoch 6/100\n255/255 [==============================] - 4s 16ms/step - loss: 0.2918 - accuracy: 0.8769 - auc: 0.9348 - val_loss: 0.2624 - val_accuracy: 0.8846 - val_auc: 0.9458\nEpoch 7/100\n255/255 [==============================] - 4s 17ms/step - loss: 0.2544 - accuracy: 0.8942 - auc: 0.9510 - val_loss: 0.2305 - val_accuracy: 0.9038 - val_auc: 0.9595\nEpoch 8/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.2204 - accuracy: 0.9082 - auc: 0.9634 - val_loss: 0.2264 - val_accuracy: 0.8930 - val_auc: 0.9606\nEpoch 9/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.1853 - accuracy: 0.9240 - auc: 0.9740 - val_loss: 0.1913 - val_accuracy: 0.9224 - val_auc: 0.9717\nEpoch 10/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.1663 - accuracy: 0.9351 - auc: 0.9784 - val_loss: 0.1890 - val_accuracy: 0.9215 - val_auc: 0.9704\nEpoch 11/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.1438 - accuracy: 0.9427 - auc: 0.9838 - val_loss: 0.1703 - val_accuracy: 0.9298 - val_auc: 0.9779\nEpoch 12/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.1344 - accuracy: 0.9502 - auc: 0.9857 - val_loss: 0.1588 - val_accuracy: 0.9313 - val_auc: 0.9807\nEpoch 13/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.1175 - accuracy: 0.9559 - auc: 0.9890 - val_loss: 0.1395 - val_accuracy: 0.9460 - val_auc: 0.9835\nEpoch 14/100\n255/255 [==============================] - 4s 16ms/step - loss: 0.1155 - accuracy: 0.9581 - auc: 0.9889 - val_loss: 0.1592 - val_accuracy: 0.9362 - val_auc: 0.9806\nEpoch 15/100\n255/255 [==============================] - 4s 16ms/step - loss: 0.1053 - accuracy: 0.9593 - auc: 0.9911 - val_loss: 0.1414 - val_accuracy: 0.9455 - val_auc: 0.9846\nEpoch 16/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.0930 - accuracy: 0.9622 - auc: 0.9927 - val_loss: 0.1652 - val_accuracy: 0.9396 - val_auc: 0.9851\nEpoch 17/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.0791 - accuracy: 0.9687 - auc: 0.9940 - val_loss: 0.0951 - val_accuracy: 0.9647 - val_auc: 0.9923\nEpoch 18/100\n255/255 [==============================] - 4s 16ms/step - loss: 0.0685 - accuracy: 0.9726 - auc: 0.9956 - val_loss: 0.1005 - val_accuracy: 0.9588 - val_auc: 0.9925\nEpoch 19/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.0553 - accuracy: 0.9802 - auc: 0.9967 - val_loss: 0.1143 - val_accuracy: 0.9602 - val_auc: 0.9910\nEpoch 20/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.1465 - accuracy: 0.9515 - auc: 0.9842 - val_loss: 0.1713 - val_accuracy: 0.9318 - val_auc: 0.9845\nEpoch 21/100\n255/255 [==============================] - 4s 15ms/step - loss: 0.0819 - accuracy: 0.9681 - auc: 0.9941 - val_loss: 0.1040 - val_accuracy: 0.9632 - val_auc: 0.9911\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = model.evaluate(X_test, y_test, verbose=0)\nprint('Test Accuracy: {:.2f}%'.format(results[1]))\nprint('     Test AUC: {:.4f}'.format(results[2]))","execution_count":25,"outputs":[{"output_type":"stream","text":"Test Accuracy: 0.97%\n     Test AUC: 0.9924\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}